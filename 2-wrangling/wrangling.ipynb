{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Data Wrangling\n",
    "---\n",
    "## Papers Past Topic Modeling\n",
    "\n",
    "<br/>\n",
    "\n",
    "Ben Faulks - bmf43@uclive.ac.nz\n",
    "\n",
    "Xiandong Cai - xca24@uclive.ac.nz\n",
    "\n",
    "Yujie Cui - ycu23@uclive.ac.nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.name', 'local'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.driver.port', '38900'),\n",
      " ('spark.driver.memory', '62g'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.driver.host', '192.168.1.207'),\n",
      " ('spark.app.id', 'local-1547627998888'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.driver.cores', '6'),\n",
      " ('spark.driver.maxResultSize', '4g')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.207:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=local>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "sys.path.insert(0, '../utils') # for import customed modules\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from utils_data import conf_pyspark, load_dataset\n",
    "\n",
    "# intiate PySpark\n",
    "sc, spark = conf_pyspark()\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load raw dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (16731578, 6)\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      id|                 url|         publisher|                time|               title|             content|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|16657613|http://api.digita...|Poverty Bay Herald|1917-02-26T00:00:...|KILLED IN MINE SH...|KILLED IN MINE SH...|\n",
      "|16707091|http://api.digita...|Poverty Bay Herald|1918-03-09T00:00:...|MUTINY IN POLISH ...|MUTINY IN POLISH ...|\n",
      "|16810288|http://api.digita...|Poverty Bay Herald|1920-02-06T00:00:...|LATE NEW ZEALAND ...|LATE NEW ZEALAND ...|\n",
      "| 7987898|http://api.digita...|Poverty Bay Herald|1891-10-19T00:00:...|Page 2 Advertisem...|rummer Fashions. ...|\n",
      "| 8192651|http://api.digita...|Poverty Bay Herald|1891-07-21T00:00:...|PARIS, July 20. (...|PARIS, July 20.Ou...|\n",
      "| 1870085|http://api.digita...|      Bruce Herald|1866-05-03T00:00:...|Original Correspo...|Original Correspo...|\n",
      "| 2726680|http://api.digita...| Ohinemuri Gazette|1892-08-06T00:00:...|Page 4 Advertisem...|NOTICE OF APPLICA...|\n",
      "| 5237356|http://api.digita...| Ohinemuri Gazette|1908-07-27T00:00:...|SOUTH AUCKLAND RI...|SOUTH AUCKLAND RI...|\n",
      "|12492137|http://api.digita...|   Southland Times|1902-06-28T00:00:...|Racing. (Southlan...|b\"Racing.(Per Uni...|\n",
      "|10687316|http://api.digita...|       Thames Star|1894-04-28T00:00:...|Page 4 Advertisem...|SUITS! SUITS I SU...|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset('papers_past', spark)\n",
    "\n",
    "nrow_raw = df.count()\n",
    "print('Shape of dataframe: ({}, {})'.format(nrow_raw, len(df.columns)))\n",
    "df.sample(False, 0.00001).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check empty values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null:\n",
      "+---+---+---------+----+-----+-------+\n",
      "| id|url|publisher|time|title|content|\n",
      "+---+---+---------+----+-----+-------+\n",
      "|  0|  0|        0|   0|    0|  56232|\n",
      "+---+---+---------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null\n",
    "print('Print Null:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop rows with empty document:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NA to avoid nonetype.\n",
    "df = df.na.drop(subset=['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null:\n",
      "+---+---+---------+----+-----+-------+\n",
      "| id|url|publisher|time|title|content|\n",
      "+---+---+---------+----+-----+-------+\n",
      "|  0|  0|        0|   0|    0|      0|\n",
      "+---+---+---------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null\n",
    "print('Print Null:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Duplicate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The \"id\" should be unique, check duplication:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated id number:  543700\n"
     ]
    }
   ],
   "source": [
    "print('Duplicated id number: ', df.count() - df.select('id').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are duplicated \"id\" in the dataset, show three of them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|      id|count|\n",
      "+--------+-----+\n",
      "|10036037|    2|\n",
      "|10059447|    2|\n",
      "|10099968|    2|\n",
      "+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('id').count().where(F.col('count')>1).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the first one to check detail:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      id|                 url|         publisher|                time|               title|             content|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|10036037|http://api.digita...|Poverty Bay Herald|1898-01-06T00:00:...|THE EASTERN SITUA...|THE EASTERN SITUA...|\n",
      "|10036037|http://api.digita...|Poverty Bay Herald|1898-01-06T00:00:...|THE EASTERN SITUA...|THE EASTERN SITUA...|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.id == 10036037).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check difference of the content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  0.9994846688997681\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "str1 = df.filter(df.id == 10036037).select('content').collect()[0]['content']\n",
    "#print(str1 + '\\n')\n",
    "\n",
    "str2 = df.filter(df.id == 10036037).select('content').collect()[1]['content']\n",
    "#print(str2 + '\\n')\n",
    "\n",
    "diff = difflib.SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "print('Similarity: ', diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The two duplicates are very close, drop one of them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check duplicate again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated id number:  0\n"
     ]
    }
   ],
   "source": [
    "print('Duplicated id number: ', df.count() - df.select('id').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Abnormal Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are should have 68 publishers, check numbers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "Correct! no abnormal values in publishers.\n"
     ]
    }
   ],
   "source": [
    "n = df.select('publisher').distinct().count()\n",
    "print(n)\n",
    "if n == 68:\n",
    "    print('Correct! no abnormal values in publishers.')\n",
    "else:\n",
    "    print('Error! abnormal values in publishers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For history documents, it only need date as time unit, we extract \"date\" column from \"time\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature date\n",
    "df = df.withColumn('date', df['time'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check schema of the dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check date range has abnormal values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(1839, 8, 21), datetime.date(1945, 12, 31))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = df.select(F.min('date'), F.max('date')).first()\n",
    "start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advertisements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check \"title\" column to see if it is possible to extract features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------+\n",
      "|id      |title                                                             |\n",
      "+--------+------------------------------------------------------------------+\n",
      "|24123878|Page 4 Advertisements Column 4 (Otago Daily Times, 19 August 1898)|\n",
      "|10804176|PARLIAMENTARY. (Grey River Argus, 09 October 1896)                |\n",
      "|19015395|Page 12 Advertisements Column 5 (Evening Post, 04 August 1925)    |\n",
      "|23545190|SPORTING. (Feilding Star, 27 May 1910)                            |\n",
      "|6489339 |Page 1 Advertisements Column 1 (Poverty Bay Herald, 27 May 1886)  |\n",
      "|23991783|THE WEATHER AND THE CROPS. (Otago Daily Times, 08 February 1892)  |\n",
      "|28298850|Turc Bell Skirt. (Auckland Star, 30 September 1893)               |\n",
      "|28135530|ANTI-CHINESE LEGISLATION. (Auckland Star, 22 March 1888)          |\n",
      "|10994388|PROSPECTING NEAR GISBORNE. (Thames Star, 15 October 1895)         |\n",
      "|17111566|HIS MAJESTY'S THEATRE. (Evening Post, 17 May 1918)                |\n",
      "+--------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.00001).limit(10).select('id', 'title').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The \"title\" column specified advertisement, we extract \"ads\" column from \"title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature ads\n",
    "df = df.withColumn('ads', df.title.contains('dvertisement'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The title consists of three parts: \"real title\" (\"publisher\", \"date\"), we only need \"real title\" part. Extract real title:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redandunt parts of title\n",
    "df = df.withColumn('title_', F.regexp_extract(F.col('title'), '(.*)(\\s\\(.*\\))', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if some titles are not the form \"title (\"publisher\", \"date\"), which will lead to \"title_\" column is empty string:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------------------+\n",
      "|id      |title_|title                |\n",
      "+--------+------+---------------------+\n",
      "|3656781 |      |Untitled Illustration|\n",
      "|4832017 |      |Untitled Illustration|\n",
      "|5417742 |      |Untitled Illustration|\n",
      "|12676570|      |Untitled Illustration|\n",
      "|12777321|      |Untitled Illustration|\n",
      "+--------+------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(F.col('title_') == '').select(['id', 'title_', 'title']).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change empty string in \"title_\" column to \"Untitled Illustration\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'title_',\n",
    "    F.when(\n",
    "        F.col('title_') == '',\n",
    "        F.lit('Untitled Illustration')\n",
    "    ).otherwise(\n",
    "        F.col('title_')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check empty string again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null:\n",
      "+---+---+---------+----+-----+-------+----+---+------+\n",
      "| id|url|publisher|time|title|content|date|ads|title_|\n",
      "+---+---+---------+----+-----+-------+----+---+------+\n",
      "|  0|  0|        0|   0|    0|      0|   0|  0|     0|\n",
      "+---+---+---------+----+-----+-------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null\n",
    "print('Print Null:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the website of [Papers Past](https://paperspast.natlib.govt.nz), we could find the publisher-region relationship in the [Explore all newspapers](https://paperspast.natlib.govt.nz/newspapers/all#region) webpage. Based on this webpage, we could extract region feature from \"publisher\" column. Here we saved [the webpage](https://paperspast.natlib.govt.nz/newspapers/all#region) and crawling the publisher-region relationship into a dataframe for extract feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# read webpage\n",
    "path = r'../temp/Papers Past _ Explore all newspapers.html'\n",
    "with open(path, 'r') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# get table \n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "table = soup.find('table', attrs={'class':'table datatable'})\n",
    "table_rows = table.find_all('tr')\n",
    "res = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [tr.text.strip() for tr in td if tr.text.strip()]\n",
    "    if row:\n",
    "        res.append(row)\n",
    "\n",
    "# transform table to pandas dataframe\n",
    "df_region = pd.DataFrame(res, columns=['publisher_', 'region', 'start_', 'end_']) # column_ means it will be drop later\n",
    "\n",
    "# transform pandas dataframe to pyspark dataframe\n",
    "df_region = spark.createDataFrame(df_region).orderBy('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (148, 4)\n",
      "+--------------------------------------+-----------------+------+----+\n",
      "|publisher_                            |region           |start_|end_|\n",
      "+--------------------------------------+-----------------+------+----+\n",
      "|Auckland Times                        |Auckland         |1842  |1846|\n",
      "|Press                                 |Canterbury       |1861  |1945|\n",
      "|Temuka Leader                         |Canterbury       |1878  |1932|\n",
      "|Timaru Herald                         |Canterbury       |1864  |1920|\n",
      "|Rangitikei Advocate and Manawatu Argus|Manawatu-Wanganui|1907  |1920|\n",
      "|Wanganui Herald                       |Manawatu-Wanganui|1867  |1920|\n",
      "|Woodville Examiner                    |Manawatu-Wanganui|1883  |1920|\n",
      "|Evening Star                          |Otago            |1865  |1947|\n",
      "|Lake Wakatip Mail                     |Otago            |1863  |1947|\n",
      "|Mt Benger Mail                        |Otago            |1881  |1941|\n",
      "|Otago Daily Times                     |Otago            |1861  |1950|\n",
      "|Southern Cross                        |Otago            |1893  |1920|\n",
      "|Samoa Times and South Sea Advertiser  |Samoa            |1888  |1896|\n",
      "|Te Aroha News                         |Waikato          |1883  |1925|\n",
      "|Thames Guardian and Mining Record     |Waikato          |1871  |1872|\n",
      "|Mareikura                             |Wellington       |1911  |1913|\n",
      "+--------------------------------------+-----------------+------+----+\n",
      "only showing top 16 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ({}, {})'.format(df_region.count(), len(df_region.columns)))\n",
    "df_region.sample(False, 0.1).show(16, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that in this publisher-region relationship dataframe, there are two publisher's name is not identical with the dataset: \"Bay Of Plenty Times\" mismatch by \"of\", \"New Zealand Free Lance\" mismatch by \"New Zeland\", so we modify the** `df_region` **to make it identical with dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update df_region for Bay Of Plenty Times and New Zealand Free Lance\n",
    "df_region = df_region.withColumn(\n",
    "    'publisher_',\n",
    "    F.when(\n",
    "        F.col('publisher_') == 'Bay of Plenty Times',\n",
    "        F.lit('Bay Of Plenty Times')\n",
    "    ).otherwise(\n",
    "        F.col('publisher_')\n",
    "    )\n",
    ").withColumn(\n",
    "    'publisher_',\n",
    "    F.when(\n",
    "        F.col('publisher_') == 'Free Lance',\n",
    "        F.lit('New Zealand Free Lance')\n",
    "    ).otherwise(\n",
    "        F.col('publisher_')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if the two publishers' name were modified:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+------+----+\n",
      "|publisher_            |region       |start_|end_|\n",
      "+----------------------+-------------+------+----+\n",
      "|Bay Of Plenty Times   |Bay of Plenty|1872  |1949|\n",
      "|New Zealand Free Lance|Wellington   |1900  |1920|\n",
      "+----------------------+-------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_region.filter((df_region.publisher_ == 'Bay Of Plenty Times') | (df_region.publisher_ == 'New Zealand Free Lance')).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract region column, and abandon redundant columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.join(df_region, df.publisher == df_region.publisher_, how='left')\n",
    "      .select(F.col('id'), \n",
    "              F.col('publisher'), \n",
    "              F.col('region'), \n",
    "              F.col('date'), \n",
    "              F.col('ads'), \n",
    "              F.col('title'), \n",
    "              F.col('content'))\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputing missing value in region column with \"unknwon\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill({'region':'unknown'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if miss any field or element:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null and empty string:\n",
      "+---+---------+------+----+---+-----+-------+\n",
      "| id|publisher|region|date|ads|title|content|\n",
      "+---+---------+------+----+---+-----+-------+\n",
      "|  0|        0|     0|   0|  0|    0|      0|\n",
      "+---+---------+------+----+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Print Null and empty string:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check dataframe szie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (16131646, 7)\n",
      "usable line percentage: 0.9641437287026962\n",
      "removed line number: 599932\n"
     ]
    }
   ],
   "source": [
    "nrow = df.count()\n",
    "print('Shape of dataframe: ({}, {})'.format(nrow, len(df.columns)))\n",
    "print('usable line percentage:', nrow/nrow_raw)\n",
    "print('removed line number:', nrow_raw - nrow)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After data wrangling, there are:**\n",
    "* 16,131,646 (96.4%) samples/rows/lines/documents usable, \n",
    "* 599,932 samples/rows/lines/documents were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Save Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataframe would be our final dataset to deal with, let's save it as compressed csv file to save time for later processes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/dataset'\n",
    "df = df.orderBy('id')\n",
    "df.write.csv(path, mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the clean dataset size:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw   dataset size: 33G\n",
      "clean dataset size: 14G\n"
     ]
    }
   ],
   "source": [
    "path = r'../data/papers_past'\n",
    "print('raw   dataset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))\n",
    "path = r'../data/dataset'\n",
    "print('clean dataset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After processing and compressing, the dataset reduce from 33GB to 14GB.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
